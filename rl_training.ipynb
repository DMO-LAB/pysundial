{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491b6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import cantera as ct\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import SundialsPy as SP\n",
    "\n",
    "# Import your utility functions\n",
    "from utils import create_solver, get_initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da565c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratorSwitchingEnv(gym.Env):\n",
    "    \"\"\"RL Environment for combustion integrator switching\"\"\"\n",
    "    \n",
    "    def __init__(self, mechanism_file='h2o2.yaml', \n",
    "                 temp_range=(800, 1200), Z_range=(0, 1.0), \n",
    "                 time_range=(1e-3, 2e-2), timestep=1e-6, super_steps=100,\n",
    "                 fuel='H2', oxidizer='O2:0.21, N2:0.79', pressure=ct.one_atm,\n",
    "                 small_Z_range=(1e-10, 1e-5), large_Z_range=(0.9999, 1.000),\n",
    "                 pressure_factor_range=(0.1, 10.0)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Simulation parameter ranges\n",
    "        self.mechanism_file = mechanism_file\n",
    "        self.temp_range = temp_range\n",
    "        self.Z_range = Z_range\n",
    "        self.time_range = time_range\n",
    "        self.timestep = timestep\n",
    "        self.super_steps = super_steps\n",
    "        self.fuel = fuel\n",
    "        self.oxidizer = oxidizer\n",
    "        self.pressure = pressure\n",
    "        self.small_Z_range = small_Z_range\n",
    "        self.large_Z_range = large_Z_range\n",
    "        self.pressure_factor_range = pressure_factor_range\n",
    "        # Current episode parameters (will be randomized)\n",
    "        self.current_temp = None\n",
    "        self.current_Z = None\n",
    "        self.current_phi = None\n",
    "        self.current_total_time = None\n",
    "        self.current_n_episodes = None\n",
    "        \n",
    "        # Action space: 6 actions (2 solvers Ã— 3 tolerance levels)\n",
    "        self.action_space = spaces.Discrete(18)\n",
    "        self.actions = [\n",
    "            \n",
    "            ('cvode_adams', 1e-6, 1e-8, None),   # BDF low tolerance \n",
    "            ('cvode_adams', 1e-8, 1e-10, None),  # BDF medium tolerance  \n",
    "            ('cvode_adams', 1e-10, 1e-12, None), # BDF high tolerance\n",
    "            ('cpp_rk23', 1e-6, 1e-8, None),    # C++ low tolerance\n",
    "            ('cpp_rk23', 1e-8, 1e-10, None),   # C++ medium tolerance\n",
    "            ('cpp_rk23', 1e-10, 1e-12, None),  # C++ high tolerance\n",
    "            ('arkode_erk', 1e-6, 1e-8, SP.arkode.ButcherTable.HEUN_EULER_2_1_2),  # Heun low tolerance\n",
    "            ('arkode_erk', 1e-8, 1e-10, SP.arkode.ButcherTable.HEUN_EULER_2_1_2),  # Heun medium tolerance\n",
    "            ('arkode_erk', 1e-10, 1e-12, SP.arkode.ButcherTable.HEUN_EULER_2_1_2),  # Heun high tolerance\n",
    "            ('cvode_bdf', 1e-6, 1e-8, None),   # BDF low tolerance\n",
    "            ('cvode_bdf', 1e-8, 1e-10, None),  # BDF medium tolerance  \n",
    "            ('cvode_bdf', 1e-10, 1e-12, None), # BDF high tolerance\n",
    "            ('arkode_erk', 1e-6, 1e-8, SP.arkode.ButcherTable.BOGACKI_SHAMPINE_4_2_3),  # Heun low tolerance\n",
    "            ('arkode_erk', 1e-8, 1e-10, SP.arkode.ButcherTable.BOGACKI_SHAMPINE_4_2_3),  # Heun medium tolerance\n",
    "            ('arkode_erk', 1e-10, 1e-12, SP.arkode.ButcherTable.BOGACKI_SHAMPINE_4_2_3),  # Heun high tolerance\n",
    "            ('arkode_dirk', 1e-6, 1e-8, SP.arkode.ButcherTable.TRBDF2_3_3_2),  # Heun low tolerance\n",
    "            ('arkode_dirk', 1e-8, 1e-10, SP.arkode.ButcherTable.TRBDF2_3_3_2),  # Heun medium tolerance\n",
    "            ('arkode_dirk', 1e-10, 1e-12, SP.arkode.ButcherTable.TRBDF2_3_3_2)  # Heun high tolerance\n",
    "        ]\n",
    "        \n",
    "        # State space: temperature + key species (O, H, OH, H2O, O2, H2)\n",
    "        self.state_species = ['O', 'H', 'OH', 'H2O', 'O2', 'H2']\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-50, high=50, shape=(8,), dtype=np.float32  # 1 temp + 6 species + 1 Z\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _setup_chemistry(self, temperature, end_time, Z, pressure_factor):\n",
    "        \"\"\"Initialize chemistry with randomized parameters\"\"\"\n",
    "        # Randomize parameters for this episode\n",
    "        if temperature is None:\n",
    "            self.current_temp = np.random.uniform(*self.temp_range)\n",
    "        else:\n",
    "            self.current_temp = temperature\n",
    "        if Z is None:\n",
    "            if np.random.uniform() < 0.25:\n",
    "                self.current_Z = np.random.uniform(*self.small_Z_range)\n",
    "            elif np.random.uniform() < 0.5:\n",
    "                self.current_Z = np.random.uniform(*self.large_Z_range)\n",
    "            else:\n",
    "                self.current_Z = np.random.uniform(*self.Z_range)\n",
    "        else:\n",
    "            self.current_Z = Z\n",
    "        if pressure_factor is None:\n",
    "            if np.random.uniform() < 0.5:\n",
    "                self.current_pressure_factor = np.random.uniform(*self.pressure_factor_range)\n",
    "            else:\n",
    "                self.current_pressure_factor = 1\n",
    "        else:\n",
    "            self.current_pressure_factor = pressure_factor\n",
    "        \n",
    "        if end_time is None:\n",
    "            self.current_total_time = np.random.uniform(*self.time_range)\n",
    "        else:\n",
    "            self.current_total_time = end_time\n",
    "        self.current_n_episodes = int(self.current_total_time / self.timestep) // self.super_steps\n",
    "        \n",
    "        # Setup separate gas objects for reference and RL\n",
    "        self.gas_ref = ct.Solution(self.mechanism_file)\n",
    "        self.gas_ref.set_mixture_fraction(self.current_Z, self.fuel, self.oxidizer)\n",
    "        self.gas_ref.TP = self.current_temp, self.pressure * self.current_pressure_factor\n",
    "        self.current_phi = self.gas_ref.equivalence_ratio(self.fuel, self.oxidizer)\n",
    "        \n",
    "        self.gas_rl = ct.Solution(self.mechanism_file)\n",
    "        self.gas_rl.set_mixture_fraction(self.current_Z, self.fuel, self.oxidizer)\n",
    "        self.gas_rl.TP = self.gas_ref.T, self.gas_ref.P\n",
    "        print(self.gas_rl.P)\n",
    "        self.current_phi = self.gas_rl.equivalence_ratio(self.fuel, self.oxidizer)\n",
    "        # Get species indices\n",
    "        self.species_indices = {spec: self.gas_ref.species_index(spec) \n",
    "                               for spec in self.state_species}\n",
    "        \n",
    "        # Run reference trajectory for this episode\n",
    "        self._run_reference_trajectory()\n",
    "    \n",
    "    def _run_reference_trajectory(self):\n",
    "        \"\"\"Run complete reference trajectory with Cantera using current parameters\"\"\"\n",
    "        reactor = ct.IdealGasConstPressureReactor(self.gas_ref)\n",
    "        sim = ct.ReactorNet([reactor])\n",
    "        sim.rtol = 1e-12\n",
    "        sim.atol = 1e-14\n",
    "        \n",
    "        self.ref_states = []\n",
    "        self.ref_times = []\n",
    "        \n",
    "        # Store initial state\n",
    "        state = np.hstack([reactor.T, reactor.thermo.Y])\n",
    "        self.ref_states.append(state)\n",
    "        self.ref_times.append(0.0)\n",
    "        \n",
    "        # Store every substep for detailed comparison\n",
    "        current_time = 0.0\n",
    "        total_substeps = self.current_n_episodes * self.super_steps\n",
    "        \n",
    "        for substep in range(total_substeps):\n",
    "            current_time += self.timestep\n",
    "            sim.advance(current_time)\n",
    "            \n",
    "            state = np.hstack([reactor.T, reactor.thermo.Y])\n",
    "            self.ref_states.append(state)\n",
    "            self.ref_times.append(current_time)\n",
    "        \n",
    "        self.ref_states = np.array(self.ref_states)\n",
    "        self.ref_times = np.array(self.ref_times)\n",
    "    \n",
    "    def _get_observation(self, state):\n",
    "        \"\"\"Transform state to observation\"\"\"\n",
    "        temp = state[0]\n",
    "        species_concentrations = state[1:]\n",
    "        \n",
    "        # Extract key species\n",
    "        obs_species = [species_concentrations[self.species_indices[spec]] \n",
    "                      for spec in self.state_species]\n",
    "        \n",
    "        # Transform: temp, 5th root of species concentrations\n",
    "        temp_norm = temp / 2000  # Normalize around initial temp\n",
    "        species_norm = np.power(np.maximum(obs_species, 1e-20), 0.2)  # 5th root\n",
    "        \n",
    "        return np.hstack([temp_norm, species_norm, self.current_Z]).astype(np.float32)\n",
    "    \n",
    "    def reset(self, seed=None, options=None, **kwargs):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        temperature = kwargs.get('temperature', None)\n",
    "        end_time = kwargs.get('end_time', None)\n",
    "        Z = kwargs.get('Z', None)\n",
    "        pressure_factor = kwargs.get('pressure_factor', None)\n",
    "        self._setup_chemistry(temperature, end_time, Z, pressure_factor)\n",
    "        self.current_episode = 0\n",
    "        self.current_state = get_initial_state(self.gas_rl)\n",
    "        self.action_history = []\n",
    "        self.action_distribution = {}\n",
    "        # Initialize trajectory storage\n",
    "        self.rl_trajectory = [self.current_state.copy()]\n",
    "        self.rl_times = [0.0]\n",
    "        self.episode_cpu_times = 0\n",
    "        self.episode_errors = 0\n",
    "        obs = self._get_observation(self.current_state)\n",
    "        self.success_count = 0\n",
    "        return obs, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = int(action)\n",
    "        \"\"\"Execute one super-timestep with chosen integrator\"\"\"\n",
    "        if self.current_episode >= self.current_n_episodes:\n",
    "            return self._get_observation(self.current_state), 0, True, True, {}\n",
    "        \n",
    "        method, rtol, atol, table_id = self.actions[action]\n",
    "        self.action_history.append(action)\n",
    "        self.action_distribution[action] = self.action_distribution.get(action, 0) + 1\n",
    "        # Integrate for super_steps using chosen method\n",
    "        t_start = self.current_episode * self.super_steps * self.timestep\n",
    "        reward, success, info = self._integrate_super_step(method, rtol, atol, table_id, t_start)\n",
    "        \n",
    "        self.current_episode += 1\n",
    "        terminated = self.current_episode >= self.current_n_episodes\n",
    "        \n",
    "        obs = self._get_observation(self.current_state)\n",
    "        self.episode_cpu_times += info['cpu_time'] if info['cpu_time'] is not None else 0\n",
    "        self.episode_errors += info['error'] if info['error'] is not None else 0\n",
    "        self.success_count += 1 if success else 0\n",
    "        info = {\n",
    "            'success': success, \n",
    "            'action': action,\n",
    "            'temp': self.current_temp,\n",
    "            'Z': self.current_Z,\n",
    "            'phi': self.current_phi,\n",
    "            'total_time': self.current_total_time,\n",
    "            'error': info['error'] if info['error'] is not None else 0,\n",
    "            'cpu_time': info['cpu_time'] if info['cpu_time'] is not None else 0,\n",
    "            'accuracy_reward': info['accuracy_reward'],\n",
    "            'efficiency_reward': info['efficiency_reward'],\n",
    "            'episode_cpu_times': self.episode_cpu_times,\n",
    "            'episode_errors': self.episode_errors,\n",
    "            'action_distribution': self.action_distribution,\n",
    "            'success_count': self.success_count\n",
    "        }\n",
    "        return obs, reward, terminated, False, info\n",
    "    \n",
    "    def _integrate_super_step(self, method, rtol, atol, table_id, t_start):\n",
    "        \"\"\"Integrate one super-timestep and calculate reward\"\"\"\n",
    "\n",
    "        t_end = t_start + (self.timestep * self.super_steps)\n",
    "        \n",
    "        try:\n",
    "            # Create solver using separate RL gas object\n",
    "            solver = create_solver(method, self.gas_rl, self.current_state, t_start, \n",
    "                                 rtol, atol, t_end=t_end, pressure=self.pressure*self.current_pressure_factor, table_id=table_id)\n",
    "            \n",
    "            # Integrate for super_steps\n",
    "            current_t = t_start\n",
    "            cpu_time = 0\n",
    "            for _ in range(self.super_steps):\n",
    "                start_time = time.time()\n",
    "                current_t += self.timestep\n",
    "                # Handle different solver types\n",
    "                if method.startswith('cpp_'):\n",
    "                    # For C++ RK solvers, use integrate() and get_y()\n",
    "                    solver.integrate(current_t)\n",
    "                    self.current_state = np.array(solver.get_y())\n",
    "                else:\n",
    "                    # For SUNDIALS and SciPy solvers, use solve_single()\n",
    "                    self.current_state = solver.solve_single(current_t)\n",
    "                cpu_time += time.time() - start_time\n",
    "                # Store RL trajectory\n",
    "                self.rl_trajectory.append(self.current_state.copy())\n",
    "                self.rl_times.append(current_t)\n",
    "            \n",
    "            \n",
    "            # Calculate reward using reference state at end of supertimestep\n",
    "            ref_state_idx = (self.current_episode + 1) * self.super_steps\n",
    "            ref_state = self.ref_states[ref_state_idx]\n",
    "            error = self._calculate_error(self.current_state, ref_state)\n",
    "            \n",
    "            # Optimized reward function with thresholds\n",
    "            accuracy_threshold = 1e-3  # Below this, accuracy doesn't matter much\n",
    "            cpu_time_threshold = 0.0005  # Below this, get great incentive\n",
    "            cpu_time_target = 0.0008  # Target CPU time\n",
    "            \n",
    "            # Accuracy reward: only penalize when above threshold\n",
    "            if error <= accuracy_threshold:\n",
    "                # Below threshold: small positive reward for being accurate\n",
    "                accuracy_reward = 2.0 * (1.0 - error / accuracy_threshold)\n",
    "            else:\n",
    "                # Above threshold: exponential penalty\n",
    "                accuracy_reward = -10.0 * np.log10(error / accuracy_threshold)\n",
    "            \n",
    "            # CPU time reward: great incentive below threshold\n",
    "            if cpu_time <= cpu_time_threshold:\n",
    "                # Below threshold: large positive reward\n",
    "                efficiency_reward = 20.0 * (1.0 - cpu_time / cpu_time_threshold)\n",
    "                #print(f\"efficiency_reward: {efficiency_reward} - cpu_time: {cpu_time} - cpu_time_threshold: {cpu_time_threshold}\")\n",
    "            elif cpu_time <= cpu_time_target:\n",
    "                # Between threshold and target: moderate reward\n",
    "                efficiency_reward = 5.0 * (1.0 - (cpu_time - cpu_time_threshold) / (cpu_time_target - cpu_time_threshold))\n",
    "            else:\n",
    "                # Above target: penalty\n",
    "                efficiency_reward = -5.0 * np.log10(cpu_time / cpu_time_target)\n",
    "            \n",
    "            # Combine rewards with weights\n",
    "            reward = 0.4 * accuracy_reward + 0.6 * efficiency_reward\n",
    "            \n",
    "            # Clip reward to reasonable range for RL stability\n",
    "            reward = np.clip(reward, -20.0, 20.0)\n",
    "            #print(f\"Reward: {reward:.3f} - Accuracy: {accuracy_reward:.3f} (error: {error:.2e}) - Efficiency: {efficiency_reward:.3f} (CPU: {cpu_time:.4f}s)\")\n",
    "            # check if reward is NaN\n",
    "            if np.isnan(reward):\n",
    "                print(f\"Reward is NaN - Accuracy Reward: {accuracy_reward} - Efficiency Reward: {efficiency_reward} - CPU Time: {cpu_time}\")\n",
    "                return -50.0, True, {\"error\": error, \"cpu_time\": cpu_time, \"accuracy_reward\": accuracy_reward, \"efficiency_reward\": efficiency_reward}\n",
    "            return reward, True, {\"error\": error, \"cpu_time\": cpu_time, \"accuracy_reward\": accuracy_reward, \"efficiency_reward\": efficiency_reward}\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            #print(f\"Integration failed: {e}\")\n",
    "            print(f\"Integration failed\")\n",
    "            return -100.0, False, {\"error\": None, \"cpu_time\": None, \"accuracy_reward\": None, \"efficiency_reward\": None}\n",
    "    \n",
    "    def _calculate_error(self, state, ref_state):\n",
    "        \"\"\"Calculate normalized error between transformed states\"\"\"\n",
    "        # Transform temperature (normalize by 2000)\n",
    "        temp_transformed = state[0] / 2000\n",
    "        ref_temp_transformed = ref_state[0] / 2000\n",
    "        temp_error = abs(temp_transformed - ref_temp_transformed)\n",
    "        #print(f\"temp_error: {temp_error} - state: {state[0]} - ref_state: {ref_state[0]}\")\n",
    "        # Species error (5th root of absolute mass fractions)\n",
    "        species_errors = []\n",
    "        for spec in self.state_species:\n",
    "            idx = self.species_indices[spec] + 1  # +1 for temperature\n",
    "            \n",
    "            # Transform species concentrations (5th root of absolute values)\n",
    "            species_transformed = np.power(np.maximum(abs(state[idx]), 1e-20), 0.2)\n",
    "            ref_species_transformed = np.power(np.maximum(abs(ref_state[idx]), 1e-20), 0.2)\n",
    "            \n",
    "            # Calculate base error\n",
    "            base_error = abs(species_transformed - ref_species_transformed)\n",
    "            #print(f\"base_error: {base_error} - state: {state[idx]} - ref_state: {ref_state[idx]}\")\n",
    "            # Add sign penalty if signs don't match (excluding very small values)\n",
    "            sign_penalty = 0\n",
    "            if abs(ref_state[idx]) > 1e-12 and abs(state[idx]) > 1e-12:\n",
    "                if np.sign(state[idx]) != np.sign(ref_state[idx]):\n",
    "                    #print(f\"sign_penalty: {sign_penalty} - state: {state[idx]} - ref_state: {ref_state[idx]}\")\n",
    "                    sign_penalty = 1.0\n",
    "            \n",
    "            total_species_error = base_error + sign_penalty\n",
    "            species_errors.append(total_species_error)\n",
    "        \n",
    "        # Combined error\n",
    "        total_error = temp_error + np.mean(species_errors)\n",
    "        return total_error\n",
    "    \n",
    "    def get_trajectory_data(self):\n",
    "        \"\"\"Extract trajectory data for plotting\"\"\"\n",
    "        if not hasattr(self, 'rl_trajectory') or len(self.rl_trajectory) == 0:\n",
    "            return None\n",
    "            \n",
    "        rl_traj = np.array(self.rl_trajectory)\n",
    "        ref_traj = self.ref_states[:len(rl_traj)]\n",
    "        \n",
    "        # Extract temperatures\n",
    "        rl_temps = rl_traj[:, 0]\n",
    "        ref_temps = ref_traj[:, 0]\n",
    "        \n",
    "        # Extract representative species\n",
    "        trajectory_data = {\n",
    "            'times': np.array(self.rl_times),\n",
    "            'ref_times': self.ref_times[:len(rl_traj)],\n",
    "            'rl_temperature': rl_temps,\n",
    "            'ref_temperature': ref_temps,\n",
    "            'rl_species': {},\n",
    "            'ref_species': {},\n",
    "            'conditions': {\n",
    "                'temperature': self.current_temp,\n",
    "                'phi': self.current_phi,\n",
    "                'total_time': self.current_total_time\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Extract species data\n",
    "        for spec in self.state_species:\n",
    "            idx = self.species_indices[spec] + 1  # +1 for temperature\n",
    "            trajectory_data['rl_species'][spec] = rl_traj[:, idx]\n",
    "            trajectory_data['ref_species'][spec] = ref_traj[:, idx]\n",
    "            \n",
    "        return trajectory_data\n",
    "\n",
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"Callback for logging training progress\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_cpu_times = []\n",
    "        self.episode_errors = []\n",
    "        self.episode_actions = []\n",
    "        self.episode_action_distribution = {}\n",
    "        self.current_episode_reward = 0\n",
    "        self.episode_success_count = []\n",
    "        \n",
    "        # Store running averages\n",
    "        self.running_avg_rewards = []\n",
    "        self.running_avg_cpu_times = []\n",
    "        self.running_avg_errors = []\n",
    "        self.running_avg_success_counts = []\n",
    "        \n",
    "        # Store action distribution over time\n",
    "        self.episode_action_distributions = []\n",
    "        self.episode_numbers = [] \n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Get the current reward\n",
    "        reward = self.locals.get('rewards', [0])[0]\n",
    "        self.current_episode_reward += reward\n",
    "        \n",
    "        # Get info from the current step\n",
    "        info = self.locals.get('infos', [{}])[0]\n",
    "        \n",
    "        # # Debug: Print info keys occasionally\n",
    "        # if len(self.episode_rewards) % 50 == 0 and len(self.episode_rewards) > 0:\n",
    "        #     print(f\"Debug - Info keys: {list(info.keys())}\")\n",
    "        \n",
    "        # Track action distribution and success count from each step\n",
    "        if 'action' in info:\n",
    "            action = info['action']\n",
    "            self.episode_action_distribution[action] = self.episode_action_distribution.get(action, 0) + 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if self.locals.get('dones', [False])[0]:\n",
    "            if 'success' in info and info['success']:\n",
    "                self.episode_success_count.append(1)\n",
    "            else:\n",
    "                self.episode_success_count.append(0)\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_cpu_times.append(info.get('episode_cpu_times', 0))\n",
    "            self.episode_errors.append(info.get('episode_errors', 0))\n",
    "            \n",
    "            # Store running averages\n",
    "            self.running_avg_rewards.append(np.mean(self.episode_rewards))\n",
    "            self.running_avg_cpu_times.append(np.mean(self.episode_cpu_times))\n",
    "            self.running_avg_errors.append(np.mean(self.episode_errors))\n",
    "            self.running_avg_success_counts.append(np.mean(self.episode_success_count))\n",
    "            \n",
    "            # Store action distribution for this episode\n",
    "            self.episode_action_distributions.append(self.episode_action_distribution.copy())\n",
    "            self.episode_numbers.append(len(self.episode_rewards))\n",
    "            \n",
    "            self.current_episode_reward = 0  # Reset for next episode\n",
    "            \n",
    "            if len(self.episode_rewards) % 10 == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards)\n",
    "                avg_cpu_time = np.mean(self.episode_cpu_times)\n",
    "                avg_error = np.mean(self.episode_errors)\n",
    "                avg_success_count = np.mean(self.episode_success_count[-10:])  # Last 10 episodes\n",
    "                average_total_cpu_time = np.mean(self.episode_cpu_times)\n",
    "                print(f\"Episode {len(self.episode_rewards)}: Avg Reward = {avg_reward:.2f} - Avg CPU Time = {avg_cpu_time:.4f} - Avg Error = {avg_error:.2e} - Average Total CPU Time = {average_total_cpu_time:.4f}\")\n",
    "                print(f\"Episode Action Distribution = {self.episode_action_distribution} - Avg Success Count = {avg_success_count:.1f}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "def train_rl_agent():\n",
    "    \"\"\"Train the RL agent\"\"\"\n",
    "    print(\"Creating environment...\")\n",
    "    env = IntegratorSwitchingEnv()\n",
    "    check_env(env)\n",
    "    \n",
    "    print(\"Training PPO agent...\")\n",
    "    callback = TrainingCallback()\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=3e-4, \n",
    "                n_steps=2048, batch_size=64, n_epochs=10)\n",
    "    \n",
    "    model.learn(total_timesteps=500000, callback=callback)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model, env, callback\n",
    "\n",
    "def evaluate_agent(model, env, n_episodes=5):\n",
    "    \"\"\"Evaluate trained agent and collect trajectory data\"\"\"\n",
    "    print(\"Evaluating agent...\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    action_counts = np.zeros(6)\n",
    "    trajectories = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        print(f'Running episode {episode + 1} with temperature {env.current_temp}K, Z {env.current_Z}, phi {env.current_phi} - pressure = {env.pressure * env.current_pressure_factor} Pa')\n",
    "        episode_reward = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            actions_taken.append(int(action))\n",
    "            action_counts[int(action)] += 1\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        trajectory_data = env.get_trajectory_data()\n",
    "        if trajectory_data:\n",
    "            trajectory_data['actions'] = actions_taken\n",
    "            trajectories.append(trajectory_data)\n",
    "            \n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, \"\n",
    "              f\"T={trajectory_data['conditions']['temperature']:.0f}K, \"\n",
    "              f\"Ï†={trajectory_data['conditions']['phi']:.2f}, Actions = {actions_taken}\\n\\n\")\n",
    "    \n",
    "    print(f\"\\nAverage reward: {np.mean(episode_rewards):.2f}\")\n",
    "    print(f\"Action distribution: {env.action_distribution}\\n\\n\")\n",
    "    \n",
    "    return episode_rewards, action_counts, trajectories\n",
    "\n",
    "def plot_training_metrics(callback, save_path=None):\n",
    "    \"\"\"Plot comprehensive training metrics\"\"\"\n",
    "    if not callback.episode_rewards:\n",
    "        print(\"No training data available\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Training Progress - All Metrics', fontsize=16)\n",
    "    \n",
    "    \n",
    "    # 1. Episode Rewards (raw and running average)\n",
    "    axes[0, 0].plot(np.arange(len(callback.episode_rewards)), callback.episode_rewards, 'b-', alpha=0.3, label='Raw Rewards')\n",
    "    axes[0, 0].plot(np.arange(len(callback.running_avg_rewards)), callback.running_avg_rewards, 'r-', linewidth=2, label='Running Average')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Reward')\n",
    "    axes[0, 0].set_title('Episode Rewards')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # 2. CPU Times (raw and running average)\n",
    "    axes[0, 1].plot(np.arange(len(callback.episode_cpu_times)), callback.episode_cpu_times, 'b-', alpha=0.3, label='Raw CPU Times')\n",
    "    axes[0, 1].plot(np.arange(len(callback.running_avg_cpu_times)), callback.running_avg_cpu_times, 'r-', linewidth=2, label='Running Average')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('CPU Time (s)')\n",
    "    axes[0, 1].set_title('Episode CPU Times')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # 3. Errors (raw and running average)\n",
    "    axes[1, 0].semilogy(np.arange(len(callback.episode_errors)), callback.episode_errors, 'b-', alpha=0.3, label='Raw Errors')\n",
    "    axes[1, 0].semilogy(np.arange(len(callback.running_avg_errors)), callback.running_avg_errors, 'r-', linewidth=2, label='Running Average')\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Error (log scale)')\n",
    "    axes[1, 0].set_title('Episode Errors')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # 4. Success Count (raw and running average)\n",
    "    axes[1, 1].plot(np.arange(len(callback.episode_success_count)), callback.episode_success_count, 'b-', alpha=0.3, label='Raw Success Count')\n",
    "    axes[1, 1].plot(np.arange(len(callback.running_avg_success_counts)), callback.running_avg_success_counts, 'r-', linewidth=2, label='Running Average')\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Success Count')\n",
    "    axes[1, 1].set_title('Episode Success Count')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # 5. Action Distribution over time\n",
    "    if callback.episode_action_distributions:\n",
    "        # Get all unique actions\n",
    "        all_actions = set()\n",
    "        for dist in callback.episode_action_distributions:\n",
    "            all_actions.update(dist.keys())\n",
    "        all_actions = sorted(list(all_actions))\n",
    "        \n",
    "        # Plot action distribution over time\n",
    "        for action in all_actions:\n",
    "            action_counts = []\n",
    "            for dist in callback.episode_action_distributions:\n",
    "                action_counts.append(dist.get(action, 0))\n",
    "            axes[2, 0].plot(np.arange(len(callback.episode_numbers)), action_counts, label=f'Action {action}', marker='o', markersize=3)\n",
    "        \n",
    "        axes[2, 0].set_xlabel('Episode')\n",
    "        axes[2, 0].set_ylabel('Action Count')\n",
    "        axes[2, 0].set_title('Action Distribution Over Time')\n",
    "        axes[2, 0].legend()\n",
    "        axes[2, 0].grid(True)\n",
    "    \n",
    "    # 6. Final Action Distribution (pie chart)\n",
    "    if callback.episode_action_distribution:\n",
    "        actions = list(callback.episode_action_distribution.keys())\n",
    "        counts = list(callback.episode_action_distribution.values())\n",
    "        axes[2, 1].pie(counts, labels=[f'Action {a}' for a in actions], autopct='%1.1f%%', startangle=90)\n",
    "        axes[2, 1].set_title('Final Action Distribution')\n",
    "    else:\n",
    "        axes[2, 1].text(0.5, 0.5, 'No action data', ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "        axes[2, 1].set_title('Final Action Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        print(f\"Training metrics plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_trajectory_comparison(trajectory_data, episode_idx=0):\n",
    "    \"\"\"Plot comparison between RL and reference trajectories\"\"\"\n",
    "    if not trajectory_data or episode_idx >= len(trajectory_data):\n",
    "        print(\"No trajectory data available\")\n",
    "        return\n",
    "        \n",
    "    traj = trajectory_data[episode_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f\"Episode {episode_idx+1}: T={traj['conditions']['temperature']:.0f}K, \"\n",
    "                 f\"Ï†={traj['conditions']['phi']:.2f}\", fontsize=14)\n",
    "    \n",
    "    # Temperature plot\n",
    "    axes[0,0].plot(traj['times'], traj['rl_temperature'], 'r-', label='RL', linewidth=2)\n",
    "    axes[0,0].plot(traj['ref_times'], traj['ref_temperature'], 'b--', label='Reference', linewidth=2)\n",
    "    axes[0,0].set_ylabel('Temperature (K)')\n",
    "    axes[0,0].set_title('Temperature')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Species plots\n",
    "    species_to_plot = ['H', 'O', 'OH', 'H2O', 'H2'][:5]  # Top 5 species\n",
    "    for i, spec in enumerate(species_to_plot):\n",
    "        row = (i + 1) // 3\n",
    "        col = (i + 1) % 3\n",
    "        \n",
    "        axes[row, col].semilogy(traj['times'], np.maximum(traj['rl_species'][spec], 1e-20), \n",
    "                               'r-', label='RL', linewidth=2)\n",
    "        axes[row, col].semilogy(traj['ref_times'], np.maximum(traj['ref_species'][spec], 1e-20), \n",
    "                               'b--', label='Reference', linewidth=2)\n",
    "        axes[row, col].set_ylabel(f'{spec} Mass Fraction')\n",
    "        axes[row, col].set_title(f'{spec} Species')\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e01a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101325.0\n"
     ]
    }
   ],
   "source": [
    "mechanism_file='large_mechanism/n-dodecane.yaml'      \n",
    "temp_range=(300, 1100)\n",
    "Z_range=(0, 1.0)\n",
    "time_range=(1e-3, 5e-2)\n",
    "timestep=1e-6\n",
    "super_steps=100\n",
    "fuel='nc12h26'\n",
    "oxidizer='O2:0.21, N2:0.79'\n",
    "pressure=ct.one_atm\n",
    "small_Z_range=(0, 1e-10)\n",
    "large_Z_range=(0.9999, 1.000)\n",
    "pressure_factor_range=(0.1, 20.0)\n",
    "env = IntegratorSwitchingEnv(mechanism_file=mechanism_file, temp_range=temp_range, \n",
    "                            Z_range=Z_range, time_range=time_range, timestep=timestep, \n",
    "                            super_steps=super_steps, fuel=fuel, oxidizer=oxidizer, pressure=pressure,\n",
    "                            small_Z_range=small_Z_range, large_Z_range=large_Z_range,\n",
    "                            pressure_factor_range=pressure_factor_range)\n",
    "\n",
    "\n",
    "state_action_pairs = []\n",
    "species_to_track = ['T', 'h', 'o2', 'h2', 'h2o', 'co2', 'co', 'oh', 'nc12h26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc50f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=3e-4, \n",
    "                n_steps=2048, batch_size=64, n_epochs=10)\n",
    "\n",
    "\n",
    "model = model.load(\"integrator_switching_ppo_20250902_002606.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # ('cvode_adams', 1e-6, 1e-8, None),   # BDF low tolerance\n",
    "            # ('cvode_adams', 1e-8, 1e-10, None),  # BDF medium tolerance  \n",
    "            # ('cvode_adams', 1e-10, 1e-12, None), # BDF high tolerance\n",
    "            # ('cpp_rk23', 1e-6, 1e-8, None),    # C++ low tolerance\n",
    "            # ('cpp_rk23', 1e-8, 1e-10, None),   # C++ medium tolerance\n",
    "            # ('cpp_rk23', 1e-10, 1e-12, None),  # C++ high tolerance\n",
    "            # ('arkode_erk', 1e-6, 1e-8, SP.arkode.ButcherTable.HEUN_EULER_2_1_2),  # Heun low tolerance\n",
    "            # ('arkode_erk', 1e-8, 1e-10, SP.arkode.ButcherTable.HEUN_EULER_2_1_2),  # Heun medium tolerance\n",
    "            # ('arkode_erk', 1e-10, 1e-12, SP.arkode.ButcherTable.HEUN_EULER_2_1_2),  # Heun high tolerance\n",
    "            # ('cvode_bdf', 1e-6, 1e-8, None),   # BDF low tolerance\n",
    "            # ('cvode_bdf', 1e-8, 1e-10, None),  # BDF medium tolerance  \n",
    "            # ('cvode_bdf', 1e-10, 1e-12, None), # BDF high tolerance\n",
    "            # ('arkode_erk', 1e-6, 1e-8, SP.arkode.ButcherTable.BOGACKI_SHAMPINE_4_2_3),  # Heun low tolerance\n",
    "            # ('arkode_erk', 1e-8, 1e-10, SP.arkode.ButcherTable.BOGACKI_SHAMPINE_4_2_3),  # Heun medium tolerance\n",
    "            # ('arkode_erk', 1e-10, 1e-12, SP.arkode.ButcherTable.BOGACKI_SHAMPINE_4_2_3),  # Heun high tolerance\n",
    "            # ('arkode_dirk', 1e-6, 1e-8, SP.arkode.ButcherTable.TRBDF2_3_3_2),  # Heun low tolerance\n",
    "            # ('arkode_dirk', 1e-8, 1e-10, SP.arkode.ButcherTable.TRBDF2_3_3_2),  # Heun medium tolerance\n",
    "            # ('arkode_dirk', 1e-10, 1e-12, SP.arkode.ButcherTable.TRBDF2_3_3_2)  # Heun high tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 1.0\n",
    "env.gas_rl.set_equivalence_ratio(phi, fuel, oxidizer)\n",
    "print(env.gas_rl.mixture_fraction(fuel, oxidizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 900\n",
    "end_time = 0.001\n",
    "Z = 0.063\n",
    "pf = 60\n",
    "obs, _ = env.reset(temperature=temperature, end_time=end_time, Z=Z, pressure_factor=pf)\n",
    "print(f\"Temperature: {obs[0]} - number of episodes: {env.current_n_episodes}\")\n",
    "\n",
    "rewards = []\n",
    "cpu_times = []\n",
    "accuracy_rewards = []\n",
    "efficiency_rewards = []\n",
    "errors = []\n",
    "action_distribution = {}\n",
    "action_distribution_list = []\n",
    "counter = 0\n",
    "while True:\n",
    "    #action = int(model.predict(obs, deterministic=True)[0])    \n",
    "    action = 17\n",
    "    action_distribution[action] = action_distribution.get(action, 0) + 1\n",
    "    action_distribution_list.append(action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    cpu_times.append(info['cpu_time'])\n",
    "    accuracy_rewards.append(info['accuracy_reward'])\n",
    "    efficiency_rewards.append(info['efficiency_reward'])\n",
    "    errors.append(info['error'])\n",
    "    print(f\"Counter: {counter} - Action: {action} - Reward: {reward} - CPU Time: {info['cpu_time']} - error: {info['error']}\")\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    counter += 1\n",
    "print(f\"Total CPU Time: {sum(cpu_times):.2f} seconds |  number of episodes: {len(rewards)}\")\n",
    "print(f\"Action distribution: {action_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 5), sharex=True, dpi=200)\n",
    "\n",
    "ax1.plot(accuracy_rewards, label='Accuracy Reward')\n",
    "ax1.plot(efficiency_rewards, label='Efficiency Reward')\n",
    "ax1.plot(rewards, label='Reward', linestyle='--')\n",
    "ax2.plot(errors, label='Error')\n",
    "ax3.plot(cpu_times, label='CPU Time')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax2.set_ylabel('Error')\n",
    "ax3.set_ylabel('CPU Time')\n",
    "ax1.set_title('Reward History')\n",
    "ax2.set_title('Error History')\n",
    "ax3.set_title('CPU Time History')\n",
    "ax4.plot(action_distribution_list, label='Action Distribution')\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Action')\n",
    "ax4.set_title('Action Distribution')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "ax4.legend()\n",
    "plt.savefig('reward_history1.png')\n",
    "plt.show()\n",
    "plot_trajectory_comparison([env.get_trajectory_data()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f713461",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gas_rl.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de543d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range((len(env.rl_trajectory)-1)//env.super_steps):\n",
    "    species_data = []\n",
    "    for spec in species_to_track:\n",
    "        if spec == 'T':\n",
    "            species_data.append(env.rl_trajectory[i*env.super_steps][0])\n",
    "        else:\n",
    "            species_data.append(env.rl_trajectory[i*env.super_steps ][env.gas_rl.species_index(spec) + 1])\n",
    "    species_data.append(env.action_history[i])\n",
    "    state_action_pairs.append([species_data])\n",
    "    counter += 1\n",
    "    \n",
    "print(len(state_action_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ced36",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_pair_np = np.array(state_action_pairs)\n",
    "state_action_pair_np.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_pair_np = state_action_pair_np.reshape(len(state_action_pairs), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e459069",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_to_track = ['T', 'h', 'o2', 'h2', 'h2o', 'co2', 'co', 'oh', 'nc12h26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abe61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot temperature vs h2o scatter plot and color by action\n",
    "plt.figure(figsize=(8, 6), dpi=200)\n",
    "plt.scatter(state_action_pair_np[:600, 0], state_action_pair_np[:600, 2], c=state_action_pair_np[:600, 9], cmap='viridis')\n",
    "plt.colorbar(label='Action')\n",
    "plt.xlabel('Temperature (K)')\n",
    "plt.ylabel('O2 Mass Fraction')\n",
    "plt.title('O2 vs Temperature Colored by Action')\n",
    "# use log scale for y axis\n",
    "# plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 5), sharex=True, dpi=200)\n",
    "\n",
    "ax1.plot(accuracy_rewards, label='Accuracy Reward')\n",
    "ax1.plot(efficiency_rewards, label='Efficiency Reward')\n",
    "ax1.plot(rewards, label='Reward', linestyle='--')\n",
    "ax2.plot(errors, label='Error')\n",
    "ax3.plot(cpu_times, label='CPU Time')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax2.set_ylabel('Error')\n",
    "ax3.set_ylabel('CPU Time')\n",
    "ax1.set_title('Reward History')\n",
    "ax2.set_title('Error History')\n",
    "ax3.set_title('CPU Time History')\n",
    "ax4.plot(action_distribution_list, label='Action Distribution')\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Action')\n",
    "ax4.set_title('Action Distribution')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "ax4.legend()\n",
    "plt.savefig('reward_history1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory_comparison([env.get_trajectory_data()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=3e-4, \n",
    "#                 n_steps=2048, batch_size=64, n_epochs=10)\n",
    "\n",
    "\n",
    "# model = model.load(\"integrator_switching_ppo\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     mechanism_file='large_mechanism/n-dodecane.yaml'      \n",
    "#     temp_range=(300, 1200)\n",
    "#     phi_range=(0.1, 2.0)\n",
    "#     time_range=(1e-3, 2e-2)\n",
    "#     timestep=1e-6\n",
    "#     super_steps=100\n",
    "#     fuel='nc12h26'\n",
    "#     oxidizer='O2:0.21, N2:0.79'\n",
    "#     pressure=ct.one_atm\n",
    "#     env = IntegratorSwitchingEnv(mechanism_file=mechanism_file, temp_range=temp_range, \n",
    "#                                 phi_range=phi_range, time_range=time_range, timestep=timestep, \n",
    "#                                 super_steps=super_steps, fuel=fuel, oxidizer=oxidizer, pressure=pressure)mechanism_file='large_mechanism/n-dodecane.yaml'      \n",
    "#     temp_range=(300, 1200)\n",
    "#     phi_range=(0.1, 2.0)\n",
    "#     time_range=(1e-3, 2e-2)\n",
    "#     timestep=1e-6\n",
    "#     super_steps=100\n",
    "#     fuel='nc12h26'\n",
    "#     oxidizer='O2:0.21, N2:0.79'\n",
    "#     pressure=ct.one_atm\n",
    "#     env = IntegratorSwitchingEnv(mechanism_file=mechanism_file, temp_range=temp_range, \n",
    "#                                 phi_range=phi_range, time_range=time_range, timestep=timestep, \n",
    "#                                 super_steps=super_steps, fuel=fuel, oxidizer=oxidizer, pressure=pressure)\n",
    "#         # Train agent\n",
    "#     model, env, callback = train_rl_agent()\n",
    "    \n",
    "#     # Plot training progress\n",
    "#     plt.figure(figsize=(10, 5), dpi=200)\n",
    "#     plt.plot(callback.episode_rewards)\n",
    "#     plt.title('Training Progress')\n",
    "#     plt.xlabel('Episode')\n",
    "#     plt.ylabel('Total Reward')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Evaluate agent and collect trajectories\n",
    "#     episode_rewards, action_counts, trajectories = evaluate_agent(model, env)\n",
    "    \n",
    "#     # Plot trajectory comparisons\n",
    "#     for i in range(min(3, len(trajectories))):  # Plot first 3 episodes\n",
    "#         plot_trajectory_comparison(trajectories, i)\n",
    "    \n",
    "#     # Save model\n",
    "#     model.save(\"integrator_switching_ppo\")\n",
    "#     print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
